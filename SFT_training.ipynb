{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Chan-Y/Stefan-Zweig-Chat\", \"default\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(dataset):\n",
    "\t\"\"\"Format the dataset to match Granite's chat template\"\"\"\n",
    "\tdef format_example(example):\n",
    "\t\tformatted_text = (\n",
    "\t\t\t\"<|start_of_role|>system<|end_of_role|>\"\n",
    "\t\t\tf\"{example['system_prompt']}<|end_of_text|>\\n\"\n",
    "\t\t\t\"<|start_of_role|>user<|end_of_role|>\"\n",
    "\t\t\tf\"{example['prompt']}<|end_of_text|>\\n\"\n",
    "\t\t\t\"<|start_of_role|>assistant<|end_of_role|><stefan_zweig>\"\n",
    "\t\t\tf\"{example['completion']}</stefan_zweig><|end_of_text|>\"\n",
    "\t\t)\n",
    "\t\treturn {\n",
    "\t\t\t\"text\": formatted_text\n",
    "\t\t}\n",
    "\t\n",
    "\treturn dataset.map(format_example)\n",
    "\n",
    "train_test_split = ds.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = format_dataset(train_test_split['train'])\n",
    "eval_dataset = format_dataset(train_test_split['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and Fine-tune Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"ibm-granite/granite-3.1-2b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': ['<stefan_zweig>', '</stefan_zweig>']\n",
    "}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lengths = []\n",
    "max_prompt_length = 0\n",
    "max_completion_length = 0\n",
    "avg_completion_length = 0\n",
    "\n",
    "for item in ds:\n",
    "\t# Calculate prompt length (system + user)\n",
    "\tprompt = (\n",
    "\t\t\"<|start_of_role|>system<|end_of_role|>\"\n",
    "\t\tf\"{item['system_prompt']}<|end_of_text|>\\n\"\n",
    "\t\t\"<|start_of_role|>user<|end_of_role|>\"\n",
    "\t\tf\"{item['prompt']}<|end_of_text|>\\n\"\n",
    "\t\t\"<|start_of_role|>assistant<|end_of_role|><stefan_zweig>\"\n",
    "\t)\n",
    "\tprompt_length = len(tokenizer.encode(prompt))\n",
    "\tmax_prompt_length = max(max_prompt_length, prompt_length)\n",
    "\t\n",
    "\t# Calculate completion length\n",
    "\tcompletion_length = len(tokenizer.encode(item['completion']))\n",
    "\tmax_completion_length = max(max_completion_length, completion_length)\n",
    "\tlengths.append(completion_length)\n",
    "\n",
    "avg_completion_length = sum(lengths) / len(lengths)\n",
    "\n",
    "print(f\"Max prompt length: {max_prompt_length}\")\n",
    "print(f\"Max completion length: {max_completion_length}\")\n",
    "print(f\"Average completion length: {avg_completion_length:.2f}\")\n",
    "print(f\"95th percentile completion length: {np.percentile(lengths, 95):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SFT_TRAIN_EPOCH = 3\n",
    "SFT_BATCH_SIZE = 4\n",
    "SFT_LR = 5e-6\n",
    "MAX_NEW_TOKEN = int(np.percentile(lengths, 95) * 1.2)\n",
    "REFERENCES = [\n",
    "\t\"\"\"My dear friend, I can sense the weight of the past bearing down upon you, much like it did upon me during those fateful years. As I reflect on the rise of Nazi Germany, I am struck by the inexorable march of ideological fanaticism and the devastating consequences that followed. It was a time when the ravages of history demonstrated, as never before, the dangers of unchecked nationalism, the deification of a particular creed, and the erosion of intellect in the face of blind dogma. As an Austrian, I was firsthand witness to the insidious creep of National Socialism, which began as a seemingly innocuous'movement' but gradually swept across Europe like a tempest, devouring everything in its path. The memories of those years still haunt me: the mustachioed orators, the cynical manipulation of the masses, the distorted echoes of a ravaged culture. My own life, once marked by the convolutions of the intellect and the pursuit of art, was caught in the maelstrom of history. As the Nazi dynasty took power, I found myself increasingly bound by the strictures of censorship and the weight of self-censorship. My relationship with my homeland, once so deeply ingrained, was sundered by the apparatus of totalitarian\"\"\",\n",
    "\t\"\"\"My dear friend, it is indeed a question that weighs heavily on the heart, particularly in the aftermath of the tumultuous 20th century. As I reflect on the masterworks of our time, I am reminded of the power of literature to be a beacon of hope, even amidst the shadows of conflict and devastation. The likes of James Joyce, Virginia Woolf, and Marcel Proust, to name but a few, offered us a mirror to the human condition, a reflection of our deepest fears and desires, and our capacity for both cruelty and compassion. Their works probed the depths of the human experience, illuminating the complexities of the human spirit, and in doing so, provided a glimmer of hope in the face of the darkness that surrounded us. The concept of the \"Lost Generation\" that emerged after the Great War serves as a powerful example of how literature became a refuge for the wounded souls of a generation. The works of Ernest Hemingway, F. Scott Fitzgerald, and T.S. Eliot, among others, validated the struggles of those who had experienced the horror of conflict and the disillusionment that followed. Their writing serves as a testament to the human spirit's capacity to persevere, to find meaning in the midst of chaos, and to\"\"\"\n",
    "\t\"\"\"Dear friend, it is a pleasure to converse with you, to explore the labyrinthine corridors of my mind, and revisit the tales that I have spun. My work, particularly \"Beware of Pity\" and \"The Post Office Girl,\" is an attempt to grapple with the complexities of human nature, the intricacies of the human condition. These novels are not so much about grand, sweeping narratives as they are about the delicate, almost imperceptible threads that bind us to one another. In \"Beware of Pity,\" I sought to explore the destructive nature of human emotions, the manner in which our deepest vulnerabilities can become the catalyst for both our downfall and our salvation. Baron von Tol Nay, the protagonist, is a self-absorbed, entitled aristocrat, while the civil servant Felix Krull is a tragic figure, forever trapped in his own vanity. Theirs is a collision course, a dance of destruction that exposes the fragility of human relationships and the devastating consequences of our own pitiless, unfeeling nature. In \"The Post Office Girl,\" I attempted to capture the dissonance between the external realities of our lives and our inner, subjective experiences. Hildegard is a young woman trapped in a narrow, suffocating existence,\"\"\",\n",
    "\t\"\"\"Dear friend, it is a sobering and eternal truth, is it not, that the human condition is a fragile and tumultuous sea, subject to the whims of fate and the vanity of human endeavor? As I sit here, reflecting on my work, I am reminded of the countless instances where the most seemingly unbreakable bonds were reduced to dust and ashes, leaving naught but the bitter taste of disillusionment and loss. You see, I have always been drawn to the intricacies of human relationships, with all their attendant complexities and contradictions. It is in the fragile balance of human emotions, in the delicate dance between love and deceit, that I find the most profound reflections of our shared humanity. The tumultuous lives of my characters, like Marie Antoinette or Fouché, may seem like worlds apart, yet they share a common thread - the fragility of human relationships that ultimately betrays us all. My work is, in a way, a lamentation of the transience of life and the impossibility of true human connection. We strive for transcendence, for beauty, for the sublime, and yet, it is in the moments of vulnerability, of fragility, that we are reminded of our fundamental isolation. Ah, but it is here, in\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "\tr=16,   \n",
    "\tlora_alpha=32,\n",
    "\ttarget_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "\tlora_dropout=0.05,\n",
    "\tbias=\"none\",\n",
    "\ttask_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA\n",
    "model.gradient_checkpointing_enable() \n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "# Example SFT training setup\n",
    "training_args = TrainingArguments(\n",
    "\toutput_dir=\"./zweig_granite_model_2301\",\n",
    "\tnum_train_epochs=SFT_TRAIN_EPOCH,\n",
    "\tper_device_train_batch_size=SFT_BATCH_SIZE,\n",
    "\tper_device_eval_batch_size=SFT_BATCH_SIZE,\n",
    "\tlearning_rate=SFT_LR,\n",
    "\tlr_scheduler_type=\"cosine\",\n",
    "\twarmup_ratio=0.1,\n",
    "\tlogging_steps=10,\n",
    "\tsave_strategy=\"epoch\",\n",
    "\tevaluation_strategy=\"epoch\",\n",
    "\tload_best_model_at_end=True,\n",
    "\tmax_grad_norm=1.0,\n",
    "\tweight_decay=0.01,\n",
    "\tfp16=True,                      \n",
    "\tgradient_checkpointing=True,     \n",
    "\tremove_unused_columns=True,      \n",
    "\tdataloader_pin_memory=True     \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "\tmodel=model,\n",
    "\targs=training_args,\n",
    "\ttrain_dataset=train_dataset,\n",
    "\teval_dataset=eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "class ZweigStyleCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.best_style_score = float('-inf')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eval_model = model\n",
    "        self.save_dir = \"./style_checkpoints\"\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            ngram_range=(1, 2),  # Use both unigrams and bigrams\n",
    "            max_features=5000     # Limit vocabulary size\n",
    "        )\n",
    "        # Fit vectorizer on reference texts\n",
    "        self.vectorizer.fit(REFERENCES)\n",
    "        \n",
    "        # Create log directory and file\n",
    "        self.log_dir = \"style_evaluation_logs\"\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        self.log_file = os.path.join(self.log_dir, f\"style_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Correct implementation of on_evaluate that matches the TrainerCallback interface\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use existing eval dataset or logs for style scoring\n",
    "            current_step = state.global_step\n",
    "            \n",
    "            # Compute style scores (potentially from existing generations)\n",
    "            style_scores = self.compute_lightweight_style_metrics()\n",
    "            \n",
    "            # Add style-related metrics to kwargs['metrics'] if it exists\n",
    "            metrics = kwargs.get('metrics', {})\n",
    "            metrics['style_score'] = np.mean(style_scores)\n",
    "            metrics['style_score_min'] = np.min(style_scores)\n",
    "            metrics['style_score_max'] = np.max(style_scores)\n",
    "            \n",
    "            # Log evaluation results\n",
    "            eval_log = {\n",
    "                'step': current_step,\n",
    "                'style_scores': style_scores,\n",
    "                'average_style_score': metrics['style_score'],\n",
    "                'best_style_score': self.best_style_score,\n",
    "                'training_loss': metrics.get('eval_loss')\n",
    "            }\n",
    "            \n",
    "            # Logging to file\n",
    "            with open(self.log_file, 'a') as f:\n",
    "                f.write(json.dumps(eval_log) + '\\n')\n",
    "            \n",
    "            # Track best score\n",
    "            if metrics['style_score'] > self.best_style_score:\n",
    "                self.best_style_score = metrics['style_score']\n",
    "                metrics['best_style_score'] = self.best_style_score\n",
    "                \n",
    "                # Save best model path\n",
    "                os.makedirs(self.save_dir, exist_ok=True)\n",
    "                best_model_path = os.path.join(self.save_dir, \"best_style_model\")\n",
    "                print(f\"New best style score {self.best_style_score:.3f}, path: {best_model_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Style evaluation error: {e}\")\n",
    "        \n",
    "        return control\n",
    "    \n",
    "    def compute_lightweight_style_metrics(self):\n",
    "        \"\"\"\n",
    "        Compute style metrics using test prompts\n",
    "        \"\"\"\n",
    "        test_prompts = [\n",
    "            \"What is your idea of the perfect day?\",\n",
    "            \"Stefan Zweig, it's an honor to converse with you. Your work spans many genres, but you're best known for your biographical and historical fiction novels, which often delve into the lives of famous figures such as Marie Antoinette, Mary Queen of Scots, and Balzac. What drew you to these subjects and time periods?\",\n",
    "            \"I have been inspired by your short stories, particularly 'Chess Story', which explores the intricate connections between people's pasts and present. What do you think triggers these connections between seemingly unrelated lives?\",\n",
    "            \"Why did you choose to write outside of Austria, especially when your family had a long history in Austria?\"\n",
    "        ]\n",
    "        \n",
    "        style_scores = []\n",
    "        for prompt in test_prompts:\n",
    "            generated_text = self._generate_text_for_prompt(prompt)\n",
    "            \n",
    "            # Calculate similarity with reference texts\n",
    "            generated_vector = self.vectorizer.transform([generated_text])\n",
    "            similarities = [\n",
    "                cosine_similarity(\n",
    "                    self.vectorizer.transform([ref_text]), \n",
    "                    generated_vector\n",
    "                )[0][0] \n",
    "                for ref_text in REFERENCES\n",
    "            ]\n",
    "            \n",
    "            style_scores.append(np.mean(similarities))\n",
    "        \n",
    "        return style_scores\n",
    "    \n",
    "    def _generate_text_for_prompt(self, prompt):\n",
    "        \"\"\"\n",
    "        Generate text for a given prompt\n",
    "        \"\"\"\n",
    "        formatted_input = (\n",
    "            \"<|start_of_role|>system<|end_of_role|>\"\n",
    "            \"You are Stefan Zweig, writing about the cultural atmosphere of Europe.<|end_of_text|>\\n\"\n",
    "            \"<|start_of_role|>user<|end_of_role|>\"\n",
    "            f\"{prompt}<|end_of_text|>\\n\"\n",
    "            \"<|start_of_role|>assistant<|end_of_role|><stefan_zweig>\"\n",
    "        )\n",
    "        \n",
    "        inputs = self.tokenizer(formatted_input, return_tensors=\"pt\").to(self.eval_model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.eval_model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=MAX_NEW_TOKEN,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.encode(\"<|end_of_text|>\", add_special_tokens=False)[0]\n",
    "            )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "callback = ZweigStyleCallback(tokenizer=tokenizer, model=model)\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "\tearly_stopping_patience=3,\n",
    "\tearly_stopping_threshold=0.01\n",
    ")\n",
    "\n",
    "trainer.add_callback(early_stopping_callback)\n",
    "trainer.add_callback(callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login() # write: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "input_text = \"You're surprised that your friends are emigrating in large numbers. The rise of the Nazi party is worrying you, and you are struck by the disparity between the ideals of the German people and the brutal actions of its government. As the Austrian writer Stefan Zweig, how do you see the future for your friends and family back home? I can almost hear the frantic tone in your voice when I ask this question.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutputs = model.generate(\n",
    "\t\t**inputs,\n",
    "\t\tmax_length=MAX_NEW_TOKEN,\n",
    "\t\tnum_return_sequences=1,\n",
    "\t\tdo_sample=True,\n",
    "\t\ttemperature=0.7,\n",
    "\t\ttop_p=0.9,\n",
    "\t)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be217dbe53f94776ab7b951ee470bdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784324e94c37443e8fb5bedc3bbab429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb643555f0b44138a7514aa982191deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f52371fba048afa39389be2b93c2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.1-2b-base\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"zweig_granite_model_2301/checkpoint-675\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stefan Zweig: Ah, my dear friend, it is a heavy burden to bear, this knowledge of the impending disaster that threatens our beloved Austria. The Nazi party, with its dark ideology, has taken root in the hearts of many Germans, and it seems that reason and compassion have been cast aside in favor of hate and violence. The once-vibrant cultural landscape of our country is now being replaced by an oppressive regime that seeks to erase every trace of individuality and humanity.\n",
      "\n",
      "My friends and family, once proud of their Austrian identity, now find themselves caught in the crosshairs of this brutal machine. They are being forced to choose between their loyalty to the German state and their love for their homeland. It is a choice that no one should ever have to make, yet here we are, faced with the stark reality of a divided world.\n",
      "\n",
      "As I watch my friends pack their bags and embark on their journeys to safer shores, I am filled with a sense of despair and helplessness. I see the hope in their eyes, the determination to rebuild their lives elsewhere, but also the pain of leaving behind everything they have ever known. They are not just emigrants; they are the very essence of what it means to be Austrian – creative, resilient, and deeply connected to their roots.\n",
      "\n",
      "I fear that the future for my friends and family back home will be one of isolation, fear, and constant vigilance. The Nazi regime will seek to crush any semblance of dissent, and those who dare to challenge its authority will face persecution and punishment. The once-thriving cultural scene will be silenced, and the arts will be used as tools to spread propaganda\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "input_text = \"You're surprised that your friends are emigrating in large numbers. The rise of the Nazi party is worrying you, and you are struck by the disparity between the ideals of the German people and the brutal actions of its government. As the Austrian writer Stefan Zweig, how do you see the future for your friends and family back home? I can almost hear the frantic tone in your voice when I ask this question.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutputs = model.generate(\n",
    "\t\t**inputs,\n",
    "\t\tmax_length=512,\n",
    "\t\tnum_return_sequences=1,\n",
    "\t\tdo_sample=True,\n",
    "\t\ttemperature=0.7,\n",
    "\t\ttop_p=0.9,\n",
    "\t)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text.split(input_text)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
