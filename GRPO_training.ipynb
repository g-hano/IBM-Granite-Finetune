{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True \n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "from reward import ZweigRewardFunction\n",
    "from constants import DEVICE, DATASET_NAME, TEST_SIZE, MODEL_PATH, CHECKPOINT_PATH, REFERENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Logging in with read token\")\n",
    "login(token=os.environ[\"HF_READ_TOKEN\"])\n",
    "\n",
    "print(\"Logging in with write token\")\n",
    "login(token=os.environ[\"HF_WRITE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "# Print information for each GPU\n",
    "for i in range(num_gpus):\n",
    "    gpu = torch.cuda.get_device_properties(i)\n",
    "    print(f\"\\nGPU {i}: {gpu.name}\")\n",
    "    # Total memory in GB\n",
    "    total_memory = gpu.total_memory / 1024**3\n",
    "    # Get current memory usage in GB\n",
    "    memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "    memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "    \n",
    "    print(f\"Total memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Allocated memory: {memory_allocated:.2f} GB\")\n",
    "    print(f\"Reserved memory: {memory_reserved:.2f} GB\")\n",
    "    print(f\"Free memory: {total_memory - memory_allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset_for_rl(dataset):\n",
    "    def format_example(example):\n",
    "        prompt = f\"<|start_of_role|>system<|end_of_role|>{example['system_prompt']}<|end_of_text|>\\n\"\n",
    "        prompt += f\"<|start_of_role|>user<|end_of_role|>{example['prompt']}<|end_of_text|>\\n\"\n",
    "        prompt += \"<|start_of_role|>assistant<|end_of_role|><stefan_zweig>\"\n",
    "        return {\"prompt\": prompt}\n",
    "    \n",
    "    return dataset.map(format_example, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rl_datasets():\n",
    "    ds = load_dataset(DATASET_NAME, \"default\")[\"train\"]\n",
    "    ds = ds.train_test_split(test_size=TEST_SIZE)\n",
    "    return (\n",
    "        format_dataset_for_rl(ds['train']),\n",
    "        format_dataset_for_rl(ds['test'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_and_tokenizer():\n",
    "    # Load base model (without value head)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        CHECKPOINT_PATH,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=DEVICE,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    base_model.train()\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = True    \n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"  # Critical for generation\n",
    "    \n",
    "    return base_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading models and tokenizer...\")\n",
    "model, tokenizer = load_models_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating datasets...\")\n",
    "train_dataset, eval_dataset = create_rl_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing reward function...\")\n",
    "reward_func = ZweigRewardFunction(tokenizer, REFERENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert any(p.requires_grad for p in model.parameters()), \"No trainable parameters!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training, test forward pass\n",
    "test_input = tokenizer(\"Test prompt:\", return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "\toutput = model(**test_input)\n",
    "assert output.logits.requires_grad is False, \"Unexpected gradient in test pass\"\n",
    "\n",
    "# Test reward function\n",
    "test_reward = reward_func([\"Test prompt\"], [\"Test response\"])\n",
    "assert isinstance(test_reward, torch.Tensor), \"Reward should return tensor\"\n",
    "assert test_reward.device == model.device, \"Device mismatch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_config = GRPOConfig(\n",
    "\toutput_dir=\"stefan_zweig_RL\",\n",
    "\tlearning_rate=1e-5,\n",
    "\tbeta=0.04,\n",
    "\tnum_generations=4,  # Reduced from 8\n",
    "\ttemperature=0.9,\n",
    "\tmax_prompt_length=384,  # Reduced from 512\n",
    "\tmax_completion_length=384,  # Reduced from 512\n",
    "\tper_device_train_batch_size=1,\n",
    "\tgradient_accumulation_steps=16,  # Increased to maintain batch size\n",
    "\tgradient_checkpointing=True,  # Activation checkpointing\n",
    "\toptim=\"adamw_torch_fused\",  # More memory-efficient optimizer\n",
    "\tfp16=False,\n",
    "\tbf16=True,  # Use bfloat16 instead of fp16\n",
    "\ttf32=True,  # Enable TensorFloat-32\n",
    "\treport_to=\"none\",\n",
    "\tlogging_steps=10,\n",
    "\tremove_unused_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_trainer = GRPOTrainer(\n",
    "\tmodel=model,\n",
    "\treward_funcs=reward_func,\n",
    "\targs=grpo_config,\n",
    "\ttrain_dataset=train_dataset,\n",
    "\teval_dataset=eval_dataset,\n",
    "\tprocessing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "\tgrpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
